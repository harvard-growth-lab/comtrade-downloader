{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd643f05-fb41-4d9d-94cb-6675ab29db8d",
   "metadata": {},
   "source": [
    "Utility file to validate downloaded raw Comtrade folders are complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d799f2-f455-485b-ab57-df643b911d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from downloader import ComtradeDownloader, API_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b6416e-772c-42cb-ad86-7e610abb9a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import comtradeapicall\n",
    "import glob\n",
    "\n",
    "%load_ext jupyter_black\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016c2cb2-6888-4f24-b583-e761784723f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for 1962 in S2\n",
      "No data for 1963 in S2\n",
      "No data for 1964 in S2\n",
      "No data for 1965 in S2\n",
      "No data for 1966 in S2\n",
      "No data for 1967 in S2\n",
      "No data for 1968 in S2\n",
      "No data for 1969 in S2\n",
      "No data for 1970 in S2\n",
      "No data for 1971 in S2\n",
      "No data for 1972 in S2\n",
      "No data for 1973 in S2\n",
      "No data for 1974 in S2\n",
      "No data for 1975 in S2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1976: [],\n",
       " 1977: [],\n",
       " 1978: [],\n",
       " 1979: [],\n",
       " 1980: [],\n",
       " 1981: [],\n",
       " 1982: [],\n",
       " 1983: [],\n",
       " 1984: [],\n",
       " 1985: [],\n",
       " 1986: [],\n",
       " 1987: [],\n",
       " 1988: [],\n",
       " 1989: [],\n",
       " 1990: [],\n",
       " 1991: [],\n",
       " 1992: [],\n",
       " 1993: [],\n",
       " 1994: [],\n",
       " 1995: [],\n",
       " 1996: [],\n",
       " 1997: [],\n",
       " 1998: [],\n",
       " 1999: [],\n",
       " 2000: [],\n",
       " 2001: [],\n",
       " 2002: [],\n",
       " 2003: [],\n",
       " 2004: [],\n",
       " 2005: [426],\n",
       " 2006: [426],\n",
       " 2007: [426],\n",
       " 2008: [],\n",
       " 2009: [],\n",
       " 2010: [],\n",
       " 2011: [],\n",
       " 2012: [],\n",
       " 2013: [100,\n",
       "  104,\n",
       "  108,\n",
       "  112,\n",
       "  116,\n",
       "  12,\n",
       "  120,\n",
       "  124,\n",
       "  132,\n",
       "  136,\n",
       "  140,\n",
       "  144,\n",
       "  152,\n",
       "  156,\n",
       "  170,\n",
       "  174,\n",
       "  178,\n",
       "  188,\n",
       "  191,\n",
       "  196,\n",
       "  20,\n",
       "  203,\n",
       "  204,\n",
       "  208,\n",
       "  212,\n",
       "  214,\n",
       "  218,\n",
       "  222],\n",
       " 2014: [],\n",
       " 2015: [],\n",
       " 2016: [],\n",
       " 2017: [],\n",
       " 2018: [],\n",
       " 2019: [],\n",
       " 2020: [],\n",
       " 2021: [],\n",
       " 2022: [275, 690]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generates dictionary with a list of all missing reporters that have data for a given year\n",
    "\n",
    "# ENTER IN DATA INPUTS\n",
    "year_start = 1962\n",
    "year_end = 2022\n",
    "classification_code = \"S2\"\n",
    "\n",
    "output_dir = \"/n/hausmann_lab/lab/atlas/data/\"\n",
    "raw_files_path = os.path.join(output_dir, \"raw\", classification_code)\n",
    "\n",
    "\n",
    "def partially_downloaded_list(year_start, year_end):\n",
    "    \"\"\"\n",
    "    Get information about last date raw files for the classification code\n",
    "    and year were last downloaded\n",
    "    \"\"\"\n",
    "    partial_dict = {}\n",
    "    for year in range(year_start, year_end + 1):\n",
    "        raw_file_by_year_path = os.path.join(raw_files_path, str(year))\n",
    "        df = comtradeapicall.getFinalDataBulkAvailability(\n",
    "            API_KEYS[\"Ellie\"],\n",
    "            typeCode=\"C\",\n",
    "            freqCode=\"A\",\n",
    "            clCode=classification_code,\n",
    "            period=str(year),\n",
    "            reporterCode=None,\n",
    "        )\n",
    "        if df.empty:\n",
    "            print(f\"No data for {year} in {classification_code}\")\n",
    "            continue\n",
    "        available_reporters = df[[\"reporterCode\"]][\"reporterCode\"].tolist()\n",
    "        for file in glob.glob(os.path.join(raw_file_by_year_path, \"*.gz\")):\n",
    "            # from file title, char 27:37 extract most recently updated date\n",
    "            reporter_code = int(file.split(\"/\")[-1][17:20])\n",
    "            try:\n",
    "                index_to_delete = available_reporters.index(reporter_code)\n",
    "                del available_reporters[index_to_delete]\n",
    "            except ValueError as e:\n",
    "                print(f\"{reporter_code} not in available reporters for year {year}\")\n",
    "                print(e)\n",
    "        partial_dict[year] = available_reporters\n",
    "\n",
    "    return partial_dict\n",
    "\n",
    "\n",
    "partial_dict = partially_downloaded_list(year_start, year_end)\n",
    "partial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da97ac30-63a9-4fd0-a948-087cea59ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on remaining files left to download identified by above function\n",
    "# One year at a time, enter the year and complete the download for that year\n",
    "\n",
    "classification_code = \"S2\"\n",
    "year = 2021\n",
    "\n",
    "output_dir = \"/n/hausmann_lab/lab/atlas/data/\"\n",
    "raw_files_path = os.path.join(output_dir, \"raw\", classification_code)\n",
    "\n",
    "for reporter_code in partial_dict[year]:\n",
    "    comtradeapicall.bulkDownloadFinalFile(\n",
    "        API_KEYS[\"Brendan\"],\n",
    "        os.path.join(raw_files_path, str(year)),\n",
    "        typeCode=\"C\",\n",
    "        freqCode=\"A\",\n",
    "        clCode=classification_code,\n",
    "        period=str(year),\n",
    "        reporterCode=str(reporter_code),\n",
    "        decompress=False,\n",
    "    )\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953e412f-6561-4208-b777-2cc5ca5aec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes a directory\n",
    "\n",
    "# import glob\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# def remove_tmp_dir(tmp_path):\n",
    "#     \"\"\"\n",
    "\n",
    "#     \"\"\"\n",
    "#     for f in glob.glob(os.path.join(tmp_path, \"*.gz\")):\n",
    "#         try:\n",
    "#             os.remove(f)\n",
    "#         except OSError as e:\n",
    "#             logging.info(f\"Error: {f} : {e.strerror}\")\n",
    "\n",
    "#     try:\n",
    "#         os.rmdir(tmp_path)\n",
    "#     except OSError as e:\n",
    "#         logging.info(f\"Error: {tmp_path} : {e.strerror}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62036814-d6c1-48a2-83db-683be8a609d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR  1992\n",
      "YEAR  1993\n",
      "YEAR  1994\n",
      "YEAR  1995\n",
      "YEAR  1996\n",
      "YEAR  1997\n",
      "YEAR  1998\n",
      "YEAR  1999\n",
      "YEAR  2000\n",
      "YEAR  2001\n",
      "YEAR  2002\n",
      "YEAR  2003\n",
      "YEAR  2004\n",
      "YEAR  2005\n",
      "YEAR  2006\n",
      "YEAR  2007\n",
      "YEAR  2008\n",
      "YEAR  2009\n",
      "YEAR  2010\n"
     ]
    }
   ],
   "source": [
    "# check for any corrupted downloaded files given classification code and range of years\n",
    "\n",
    "years = [1992, 2022]\n",
    "start_year = years[0]\n",
    "end_year = years[1]\n",
    "\n",
    "corrupted_files = []  # List to store names of corrupted files\n",
    "classification_code = \"H0\"\n",
    "\n",
    "output_dir = \"/n/hausmann_lab/lab/atlas/data/\"\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    raw_files_path = os.path.join(output_dir, \"raw\", classification_code, str(year))\n",
    "    print(\"YEAR \", year)\n",
    "    for file in glob.glob(os.path.join(raw_files_path, \"*.gz\")):\n",
    "        try:\n",
    "            # Try reading the CSV file with Pandas\n",
    "            pd.read_csv(file)\n",
    "        except Exception as e:\n",
    "            # If an exception occurs, consider the file corrupted and add its name to the list\n",
    "            print(f\"Error reading '{file}': {e}\")\n",
    "            corrupted_files.append(file)\n",
    "\n",
    "if corrupted_files:\n",
    "    print(\"Corrupted files:\")\n",
    "    for corrupted_file in corrupted_files:\n",
    "        print(corrupted_file)\n",
    "else:\n",
    "    print(\"No corrupted files found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
